
<!DOCTYPE html>
<html>
  <head>
    <title>no title</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
    .markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview .pagebreak,.markdown-preview .newpage{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center !important}.markdown-preview:not([for="preview"]) .code-chunk .btn-group{display:none}.markdown-preview:not([for="preview"]) .code-chunk .status{display:none}.markdown-preview:not([for="preview"]) .code-chunk .output-div{margin-bottom:16px}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0}@media screen and (min-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{font-size:14px !important;padding:1em}}@media print{html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,0.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{padding:0 1.6em;margin-top:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc li{margin-bottom:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{list-style-type:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% -  300px);padding:2em calc(50% - 457px -  150px);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}/**
 * prism.js Github theme based on GitHub's theme.
 * @author Sam Clarke
 */
code[class*="language-"],
pre[class*="language-"] {
  color: #333;
  background: none;
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.4;

  -moz-tab-size: 8;
  -o-tab-size: 8;
  tab-size: 8;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
  padding: .8em;
  overflow: auto;
  /* border: 1px solid #ddd; */
  border-radius: 3px;
  /* background: #fff; */
  background: #f5f5f5;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  padding: .1em;
  border-radius: .3em;
  white-space: normal;
  background: #f5f5f5;
}

.token.comment,
.token.blockquote {
  color: #969896;
}

.token.cdata {
  color: #183691;
}

.token.doctype,
.token.punctuation,
.token.variable,
.token.macro.property {
  color: #333;
}

.token.operator,
.token.important,
.token.keyword,
.token.rule,
.token.builtin {
  color: #a71d5d;
}

.token.string,
.token.url,
.token.regex,
.token.attr-value {
  color: #183691;
}

.token.property,
.token.number,
.token.boolean,
.token.entity,
.token.atrule,
.token.constant,
.token.symbol,
.token.command,
.token.code {
  color: #0086b3;
}

.token.tag,
.token.selector,
.token.prolog {
  color: #63a35c;
}

.token.function,
.token.namespace,
.token.pseudo-element,
.token.class,
.token.class-name,
.token.pseudo-class,
.token.id,
.token.url-reference .token.variable,
.token.attr-name {
  color: #795da3;
}

.token.entity {
  cursor: help;
}

.token.title,
.token.title .token.punctuation {
  font-weight: bold;
  color: #1d3e81;
}

.token.list {
  color: #ed6a43;
}

.token.inserted {
  background-color: #eaffea;
  color: #55a532;
}

.token.deleted {
  background-color: #ffecec;
  color: #bd2c00;
}

.token.bold {
  font-weight: bold;
}

.token.italic {
  font-style: italic;
}


/* JSON */
.language-json .token.property {
  color: #183691;
}

.language-markup .token.tag .token.punctuation {
  color: #333;
}

/* CSS */
code.language-css,
.language-css .token.function {
  color: #0086b3;
}

/* YAML */
.language-yaml .token.atrule {
  color: #63a35c;
}

code.language-yaml {
  color: #183691;
}

/* Ruby */
.language-ruby .token.function {
  color: #333;
}

/* Markdown */
.language-markdown .token.url {
  color: #795da3;
}

/* Makefile */
.language-makefile .token.symbol {
  color: #795da3;
}

.language-makefile .token.variable {
  color: #183691;
}

.language-makefile .token.builtin {
  color: #0086b3;
}

/* Bash */
.language-bash .token.keyword {
  color: #0086b3;
}

/* highlight */
pre[data-line] {
  position: relative;
  padding: 1em 0 1em 3em;
}
pre[data-line] .line-highlight-wrapper {
  position: absolute;
  top: 0;
  left: 0;
  background-color: transparent;
  display: block;
  width: 100%;
}

pre[data-line] .line-highlight {
  position: absolute;
  left: 0;
  right: 0;
  padding: inherit 0;
  margin-top: 1em;
  background: hsla(24, 20%, 50%,.08);
  background: linear-gradient(to right, hsla(24, 20%, 50%,.1) 70%, hsla(24, 20%, 50%,0));
  pointer-events: none;
  line-height: inherit;
  white-space: pre;
}

pre[data-line] .line-highlight:before, 
pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-start);
  position: absolute;
  top: .4em;
  left: .6em;
  min-width: 1em;
  padding: 0 .5em;
  background-color: hsla(24, 20%, 50%,.4);
  color: hsl(24, 20%, 95%);
  font: bold 65%/1.5 sans-serif;
  text-align: center;
  vertical-align: .3em;
  border-radius: 999px;
  text-shadow: none;
  box-shadow: 0 1px white;
}

pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-end);
  top: auto;
  bottom: .4em;
}.emoji {
  height: 0.8em;
}html body{font-family:"Helvetica Neue",Helvetica,"Segoe UI",Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ul,html body>ol{margin-bottom:16px}html body ul,html body ol{padding-left:2em}html body ul.no-list,html body ol.no-list{padding:0;list-style-type:none}html body ul ul,html body ul ol,html body ol ol,html body ol ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:bold;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:bold}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em !important;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::before,html body code::after{letter-spacing:-0.2em;content:"\00a0"}html body pre>code{padding:0;margin:0;font-size:.85em !important;word-break:normal;white-space:pre;background:transparent;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;font-size:.85em !important;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:before,html body pre tt:before,html body pre code:after,html body pre tt:after{content:normal}html body p,html body blockquote,html body ul,html body ol,html body dl,html body pre{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body pre,html body code{word-wrap:break-word;white-space:pre}}
    /* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */

    </style>
    <link rel="stylesheet" href="file:///c:\Users\mlouw\.vscode\extensions\shd101wyy.markdown-preview-enhanced-0.6.3\node_modules\@shd101wyy\mume\dependencies\katex\katex.min.css">
  </head>
  <body for="html-export">
    <div class="mume markdown-preview">
    <html><head></head><body><div><h1 class="mume-header" id="noml-learning-track-notes" ebook-toc-level-1 heading="NoML learning track notes">NoML learning track notes</h1>

<p><a href="https://weifoo.gitbooks.io/noml/content/">source site</a></p>
<div class="code-chunk" data-id="code-chunk-id-0" data-cmd="toc"><div class="input-div"><div class="btn-group"><div class="run-btn btn"><span>&#x25B6;&#xFE0E;</span></div><div class="run-all-btn btn">all</div></div><div class="status">running...</div></div><div class="output-div"></div></div><ul>
<li><a href="#bias-variance">Bias / Variance</a></li>
<li><a href="#l1-and-l2-regularization">L1 and L2 regularization</a></li>
<li><a href="#fighting-overunderfitting">Fighting over/underfitting</a>
<ul>
<li><a href="#feature-selection">Feature selection</a></li>
<li><a href="#regularization">Regularization</a></li>
<li><a href="#dimension-reduction">Dimension Reduction</a></li>
<li><a href="#model-ensemble">Model ensemble</a>
<ul>
<li><a href="#bagging">Bagging</a></li>
<li><a href="#boosting">Boosting</a>
<ul>
<li><a href="#bagging-vs-boosting">Bagging vs Boosting</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#generative-vs-discriminative">Generative vs Discriminative</a></li>
<li><a href="#svm-and-logistic-regression">SVM and Logistic Regression</a></li>
<li><a href="#classification">Classification</a>
<ul>
<li><a href="#rocprecision-recall-curves">ROC/Precision Recall curves</a>
<ul>
<li><a href="#roc">ROC</a></li>
<li><a href="#precision-recall">Precision-Recall</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#basic-fully-connected-neural-network-using-numpy">Basic fully-connected Neural network using NumPy</a></li>
</ul>
<h2 class="mume-header" id="bias-variance" ebook-toc-level-2 heading="Bias / Variance">Bias / Variance</h2>

<p>Bias error comes from <strong>wrong assumptions</strong> in the algorithm/implementation. High bias models oversimplify the data and have high error on the training data (underfitting).</p>
<p>Variance error comes from the <strong>variability of the data</strong>. High variance leads to fitting the training set too well (overfitting).</p>
<table>
<thead>
<tr>
<th>&#xFEFF;</th>
<th>Low Bias</th>
<th>High Bias</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Low Variance</strong></td>
<td><em>Ideal model</em></td>
<td><strong>Underfitting</strong></td>
</tr>
<tr>
<td><strong>High Variance</strong></td>
<td><strong>Overfitting</strong></td>
<td>Very bad</td>
</tr>
</tbody>
</table>
<h2 class="mume-header" id="l1-and-l2-regularization" ebook-toc-level-2 heading="L1 and L2 regularization">L1 and L2 regularization</h2>

<p>Regularization is used in machine learning to prevent overfitting in regression problems. It does so <strong>by adding a penalty</strong> to the model as the complexity increases. This way, higher terms of the equation become negligible and the model is less prone to overfitting.</p>
<ul>
<li>L1 regularization</li>
</ul>
<p>Also called <em>Lasso Regression</em>, it adds the absolute value of the coefficient to the cost function. This technique will shrink the less important features&apos; coefficients to zero, and give <strong>sparse estimates</strong> (feature selection effect + more storage efficient). Therefore, it is more effective with a large number of features. It is nonlinear, there is no closed formsolution.<br>
(In high dimensional spaces, you get a lot of 0s and a few non-zero coefficients).</p>
<ul>
<li>L2 regularization</li>
</ul>
<p>Also caller <em>Ridge Regression</em>, it adds the square of the coefficient as a penalty. With a large coefficient, l2 <strong>reduces the magnitude of the parameters</strong> (it shrinks the parameter towards zero). Increases bias, reduces variance. It is also quadratic in the weights vector <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> (less computational complexity than L1).</p>
<p>(cf. isosurfaces for L1 and L2: L1 will more often set features to zero because of the corners)</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&#x21D2;</mo></mrow><annotation encoding="application/x-tex">\Rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">&#x21D2;</span></span></span></span> <strong>L2 is more stable because of the square</strong><br>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&#x21D2;</mo></mrow><annotation encoding="application/x-tex">\Rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">&#x21D2;</span></span></span></span> ElasticNet is a combination of L1 and L2 regularization, &quot;should&quot; be used in practice.</p>
<h2 class="mume-header" id="fighting-overunderfitting" ebook-toc-level-2 heading="Fighting over/underfitting">Fighting over/underfitting</h2>

<p>2 main ways to manage the bias/variance tradeoff:</p>
<ol>
<li>
<p>Model selection</p>
<ul>
<li>Feature selection</li>
<li>Regularization</li>
<li>Dimension Reduction</li>
</ul>
</li>
<li>
<p>Model ensemble</p>
<ul>
<li>Bagging (reduces variance)</li>
<li>Boosting (reduces bias)</li>
</ul>
</li>
</ol>
<h3 class="mume-header" id="feature-selection" ebook-toc-level-3 heading="Feature selection">Feature selection</h3>

<p>Best subset selection = Start from a model <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">M</mi><mn mathvariant="script">0</mn></msub></mrow><annotation encoding="application/x-tex">\mathcal{M_0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathcal">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> with no features, and fit k models with 1 feature. Choose the best resulting model (smallest RSS <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&#x21D4;</mo></mrow><annotation encoding="application/x-tex">\Leftrightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">&#x21D4;</span></span></span></span> largest R&#xB2;) and repeat up to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span> features<br>
Select a <strong>single</strong> best model among <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">M</mi><mn mathvariant="script">0</mn></msub><mo separator="true">,</mo><mo>&#x2026;</mo><mo separator="true">,</mo><msub><mi mathvariant="script">M</mi><mi>M</mi></msub></mrow><annotation encoding="application/x-tex">\mathcal{M_0}, \dots ,\mathcal{M}_M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathcal">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">&#x2026;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathcal">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> models (using CV/AIC/BIC/...)<br>
Problem : <strong>Too computationally expensive + overfitting</strong></p>
<p>In practice, use of meta-heuristics:</p>
<ul>
<li>Filter (rank features, select the best ones)</li>
<li>Embedded (built-in, e.g. lasso/decision tree)</li>
<li>Wrapper (forward selection or backward selection)</li>
</ul>
<p>To choose the optimal model, RSS and R&#xB2; are nott suitable because we need to evaluate the test errror.</p>
<ul>
<li>Direct estimation with validation set (LOO cv, in practice k-fold)</li>
<li>Adjustment to training error to account for model complexity (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>I</mi><mi>C</mi></mrow><annotation encoding="application/x-tex">AIC</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>/<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>I</mi><mi>C</mi></mrow><annotation encoding="application/x-tex">BIC</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>/Adjusted <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>)</li>
</ul>
<h3 class="mume-header" id="regularization" ebook-toc-level-3 heading="Regularization">Regularization</h3>

<ul>
<li>Shrinkage methods (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">L1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mord">1</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">L2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mord">2</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>c</mi><mi>N</mi><mi>e</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">ElasticNet</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">El</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">c</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span></span></span></span>). Use CV to compute the best <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>&#x3BB;</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">&#x3BB;</span></span></span></span> parameter for regularization.</li>
</ul>
<h3 class="mume-header" id="dimension-reduction" ebook-toc-level-3 heading="Dimension Reduction">Dimension Reduction</h3>

<p>Transforms the original features and learn a model on the transformed features, whereas other methods worked on the original features.<br>
Examples: PCA, ICA, LDA, Self-organizing maps, autoencoders,...</p>
<ul>
<li>PCA</li>
</ul>
<p>Keep the most information as possible, using <em>variance</em> as a measure of it<br>
Gives a new orthogonal basis whose axes align with the max variance of the original data. Example: eigenfaces, PCA on images of faces gives us a small list of features to look for to recognize a human face. Each obtained eigenvector can be represented as an image of a &apos;face feature&apos;</p>
<p>Caveats:<br>
Fails when data has multiple clusters.<br>
Greatest variance might not mean most information.<br>
PCA is linear, but data often lies on nonlinear spaces.</p>
<h3 class="mume-header" id="model-ensemble" ebook-toc-level-3 heading="Model ensemble">Model ensemble</h3>

<p>Meta-algorithms. Instead of learning one model, learn a set of models and combine them intelligently. Typically improves accuracy by a lot.</p>
<h4 class="mume-header" id="bagging" ebook-toc-level-4 heading="Bagging">Bagging</h4>

<p>Reduces variance without increasing bias.</p>
<ol>
<li>Generate <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">B</mi></mrow><annotation encoding="application/x-tex">\mathcal{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathcal" style="margin-right:0.03041em;">B</span></span></span></span> boostrap samples of the training data (random sampling w/ replacement).</li>
<li>Train a model on each sample</li>
<li>Prediction = Majoriti vote for classification, average of prediction for reg.</li>
</ol>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&#x21D2;</mo></mrow><annotation encoding="application/x-tex">\Rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">&#x21D2;</span></span></span></span> Decreases variance due to averaging (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mi>a</mi><mi>r</mi><mo stretchy="false">(</mo><mover accent="true"><mi>X</mi><mo>&#x2C9;</mo></mover><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>V</mi><mi>a</mi><mi>r</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow><mi>N</mi></mfrac></mrow><annotation encoding="application/x-tex">Var(\bar X) = \frac{Var(X)}{N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0701em;vertical-align:-0.25em;"></span><span class="mord mathnormal">Va</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8201em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span><span style="top:-3.2523em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1667em;"><span class="mord">&#x2C9;</span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.355em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Va</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">&#x200B;</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>)<br>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&#x21D2;</mo></mrow><annotation encoding="application/x-tex">\Rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">&#x21D2;</span></span></span></span> Reduces variation (i.e. overfitting) especially for unstable learners, works particularly well with decision trees (random forest).<br>
Does not help when high bias.</p>
<h4 class="mume-header" id="boosting" ebook-toc-level-4 heading="Boosting">Boosting</h4>

<p>Reduces bias without increasing variance.</p>
<p>Sequentially train weak learners:</p>
<ol>
<li>Train model on train set</li>
<li>compute training error</li>
<li>Increase weights on train cases where model is wrong</li>
<li>Repeat training and weights update</li>
</ol>
<p>Final model = weighted prediciton of each model</p>
<p>Example models: AdaBoost for classification</p>
<h5 class="mume-header" id="bagging-vs-boosting" ebook-toc-level-5 heading="Bagging vs Boosting">Bagging vs Boosting</h5>

<table>
<thead>
<tr>
<th></th>
<th>Bagging</th>
<th>Boosting</th>
</tr>
</thead>
<tbody>
<tr>
<td>reduces</td>
<td>variance</td>
<td>bias</td>
</tr>
<tr>
<td>stable models</td>
<td><strong>not better</strong></td>
<td>might help</td>
</tr>
<tr>
<td>noisy datasets</td>
<td>no problem</td>
<td>might hurt performance</td>
</tr>
</tbody>
</table>
<p>In practice, bagging almost always good. Boosting helps more than bagging but can be detrimental as well. The weights grow exponentially.<br>
Bagging = easy to parallelize.</p>
<h2 class="mume-header" id="generative-vs-discriminative" ebook-toc-level-2 heading="Generative vs Discriminative">Generative vs Discriminative</h2>

<p><strong>Generative</strong> models learn a <strong>joint distribution</strong> over the data <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(x, y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span> and can be used to generate samples. Bayes&apos; rule allow to get the conditional distribution <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">&#x2223;</mi><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(y|x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord">&#x2223;</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> from the joint distribution.<br>
<strong>Discriminative</strong> models learn a <strong>conditional distribution</strong> over the data <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">&#x2223;</mi><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(y|x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord">&#x2223;</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> and can be used to predict the class of a sample.</p>
<p><strong>Discriminative</strong> models <strong>generally outperform generative</strong> ones in classification tasks.</p>
<h2 class="mume-header" id="svm-and-logistic-regression" ebook-toc-level-2 heading="SVM and Logistic Regression">SVM and Logistic Regression</h2>

<p>Logistic regression focuses on <strong>maximizing the likelihood</strong>. The further the data lies from the separating hyperplane (on the correct side), the happier LR is.</p>
<p>An SVM tries to find the separating hyperplane that maximizes the distance of the <strong>closest points</strong> to the margin (the support vectors). If a point is not a support vector, it doesn&#x2019;t really matter.<br>
SVM can use nonlinear kernels if data can&apos;t be split linearly. Gaussian RBF kernel is a good choice in practice.</p>
<p>We can express <a href="http://www.cs.toronto.edu/~kswersky/wp-content/uploads/svm_vs_lr.pdf#page=7">SVM as a derivation of LR</a>.</p>
<h2 class="mume-header" id="classification" ebook-toc-level-2 heading="Classification">Classification</h2>

<h3 class="mume-header" id="rocprecision-recall-curves" ebook-toc-level-3 heading="ROC/Precision Recall curves">ROC/Precision Recall curves</h3>

<p>Both ROC and precision-recall curves are used to evaluate the performance of a classifier. Usually binary, but can work with multiclass.</p>
<h4 class="mume-header" id="roc" ebook-toc-level-4 heading="ROC">ROC</h4>

<p>Used for binary classification where classes are <strong>balanced</strong>.<br>
Y-axis = <strong>true positive</strong> rate (sensitivity)<br>
X-axis = <strong>false positive</strong> rate (1 - specificity)<br>
<strong>AUC</strong> = summary of the model&apos;s skill.<br>
Can be compared between different models.<br>
Useful to choose a <strong>threshold</strong> specific to the use case.<br>
Perfect skill = point in (0, 1)</p>
<h4 class="mume-header" id="precision-recall" ebook-toc-level-4 heading="Precision-Recall">Precision-Recall</h4>

<p>Used when a <strong>lot of 0 and few ones</strong> (imbalance), because then we are less interested in false negatives<br>
<strong>Precision</strong> = True Positives / (True Positives + False Positives) (Positive Predictive Value)<br>
<strong>Recall</strong> = True Positives / (True Positives + False Negatives) (Sensitivity)<br>
Perfect skill = point in (1, 1)<br>
Can also use AUC here, and <strong>F1-score</strong> to summarize precision and recall together.</p>
<p>ROC can be <strong>overly optimistic</strong> when classes are imbalanced <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&#x21D2;</mo></mrow><annotation encoding="application/x-tex">\Rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">&#x21D2;</span></span></span></span> Precision recall curve are more appropriate.</p>
<h2 class="mume-header" id="basic-fully-connected-neural-network-using-numpy" ebook-toc-level-2 heading="Basic fully-connected Neural network using NumPy">Basic fully-connected Neural network using NumPy</h2>

<p>See <a href="file:///C:\Users\mlouw\OneDrive\Documents\Fun\ML\neural_net.py"><code>neural_net.py</code></a></p>
</div></body></html>
    </div>
  </body>
</html>
